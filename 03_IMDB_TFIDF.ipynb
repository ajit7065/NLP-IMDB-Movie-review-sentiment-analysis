{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8daa8bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"dark_background\")\n",
    "sns.set_palette(\"dark\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score,recall_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0235f60f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>one reviewers mentioned watching 1 oz episode ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>wonderful little production br br filming tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>basically family little boy jake thinks zombie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>petter mattei love time money visually stunnin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  sentiment                                         text_clean\n",
       "0           0          1  one reviewers mentioned watching 1 oz episode ...\n",
       "1           1          1  wonderful little production br br filming tech...\n",
       "2           2          1  thought wonderful way spend time hot summer we...\n",
       "3           3          0  basically family little boy jake thinks zombie...\n",
       "4           4          1  petter mattei love time money visually stunnin..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"D:\\\\Portfolio\\\\03_IMDB_reviews\\\\IMDB_Preprocessed.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ea36272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49582, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9d2b5bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    24884\n",
       "0    24698\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72730fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['sentiment'].values\n",
    "X = df[['text_clean']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e14a5108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['text_clean'].isnull()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aee304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b61aaf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34707, 1) (34707,)\n",
      "(14875, 1) (14875,)\n",
      "====================================================================================================\n",
      "After vectorizations\n",
      "(34707, 10000) (34707,)\n",
      "(14875, 10000) (14875,)\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "#Getting TF-IDF\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Handling NaN values by replacing them with an empty string\n",
    "X_train['text_clean'].fillna('', inplace=True)\n",
    "X_test['text_clean'].fillna('', inplace=True)\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=2,ngram_range=(1,4), max_features=10000)\n",
    "vectorizer.fit(X_train['text_clean'])\n",
    "X_train_tfidf = vectorizer.transform(X_train['text_clean'].values)\n",
    "X_test_tfidf = vectorizer.transform(X_test['text_clean'].values)\n",
    "print(\"After vectorizations\")\n",
    "print(X_train_tfidf.shape, y_train.shape)\n",
    "print(X_test_tfidf.shape, y_test.shape)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c007c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34707, 10000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27e15c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14875, 10000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8265425b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34707,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b89b5408",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train_tfidf.toarray()\n",
    "X_test=X_test_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e6a4e0",
   "metadata": {},
   "source": [
    "# Gausian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cedc7ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6083 1327]\n",
      " [1207 6258]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.83      7410\n",
      "           1       0.83      0.84      0.83      7465\n",
      "\n",
      "    accuracy                           0.83     14875\n",
      "   macro avg       0.83      0.83      0.83     14875\n",
      "weighted avg       0.83      0.83      0.83     14875\n",
      "\n",
      "***AUC score***\n",
      "AUC score for NB is  0.8579570880935142\n",
      "***Accuracy score***\n",
      "Train Accuracy score for NB is  0.8739447373728643\n",
      "Test Accuracy score for NB is  0.8296470588235294\n",
      "***Recall score***\n",
      "Train recall score for NB is  0.880073482978357\n",
      "Test recall score for NB is  0.838312123241795\n"
     ]
    }
   ],
   "source": [
    "# Training Naive bayes\n",
    "# Finding Accuracy, AUC, False positive rate, True positive rate, confusion matrix and classificatio report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "NB = GaussianNB()\n",
    "NB.fit(X_train, y_train)\n",
    "pred = NB.predict(X_test)\n",
    "accNB = accuracy_score(y_test, pred)\n",
    "y_pred_prob = NB.predict_proba(X_test)\n",
    "predT=NB.predict(X_train)\n",
    "aucScoreNB = roc_auc_score(y_test,  y_pred_prob[:,1])\n",
    "fprNB, tprNB, thresholds = roc_curve(y_test, y_pred_prob[:,1] )\n",
    "print(confusion_matrix(y_test,pred))\n",
    "print(classification_report(y_test,pred))\n",
    "print(\"***AUC score***\")\n",
    "print(\"AUC score for NB is \",aucScoreNB)\n",
    "print(\"***Accuracy score***\")\n",
    "print(\"Train Accuracy score for NB is \",accuracy_score(y_train, predT))\n",
    "print(\"Test Accuracy score for NB is \",accuracy_score(y_test, pred))\n",
    "print(\"***Recall score***\")\n",
    "print(\"Train recall score for NB is \",recall_score(y_train, predT))\n",
    "print(\"Test recall score for NB is \",recall_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7854150",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "183dbe88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6552  858]\n",
      " [ 692 6773]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89      7410\n",
      "           1       0.89      0.91      0.90      7465\n",
      "\n",
      "    accuracy                           0.90     14875\n",
      "   macro avg       0.90      0.90      0.90     14875\n",
      "weighted avg       0.90      0.90      0.90     14875\n",
      "\n",
      "***AUC score***\n",
      "AUC score for LR is  0.9600131608324227\n",
      "***Accuracy score***\n",
      "Train Accuracy score for LR is  0.9294378655602616\n",
      "Test Accuracy score for LR is  0.8957983193277311\n",
      "***Recall score***\n",
      "Train recall score for LR is  0.9376542855502612\n",
      "Test recall score for LR is  0.9073007367716008\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Create a Logistic Regression classifier\n",
    "LR = LogisticRegression()\n",
    "LR.fit(X_train, y_train)\n",
    "pred = LR.predict(X_test)\n",
    "accLR = accuracy_score(y_test, pred)\n",
    "y_pred_prob = LR.predict_proba(X_test)\n",
    "predT=LR.predict(X_train)\n",
    "aucScoreLR = roc_auc_score(y_test,  y_pred_prob[:,1])\n",
    "fprLR, tprLR, thresholds = roc_curve(y_test, y_pred_prob[:,1] )\n",
    "print(confusion_matrix(y_test,pred))\n",
    "print(classification_report(y_test,pred))\n",
    "print(\"***AUC score***\")\n",
    "print(\"AUC score for LR is \",aucScoreLR)\n",
    "print(\"***Accuracy score***\")\n",
    "print(\"Train Accuracy score for LR is \",accuracy_score(y_train, predT))\n",
    "print(\"Test Accuracy score for LR is \",accuracy_score(y_test, pred))\n",
    "print(\"***Recall score***\")\n",
    "print(\"Train recall score for LR is \",recall_score(y_train, predT))\n",
    "print(\"Test recall score for LR is \",recall_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a823c1",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9f9a90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5303 2107]\n",
      " [2184 5281]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.72      0.71      7410\n",
      "           1       0.71      0.71      0.71      7465\n",
      "\n",
      "    accuracy                           0.71     14875\n",
      "   macro avg       0.71      0.71      0.71     14875\n",
      "weighted avg       0.71      0.71      0.71     14875\n",
      "\n",
      "***AUC score***\n",
      "AUC score for DT is  0.7115446080810764\n",
      "***Accuracy score***\n",
      "Train Accuracy score for DT is  1.0\n",
      "Test Accuracy score for DT is  0.7115294117647059\n",
      "***Recall score***\n",
      "Train recall score for DT is  1.0\n",
      "Test recall score for DT is  0.7074346952444742\n"
     ]
    }
   ],
   "source": [
    "# Decision Trees (Multiple if-else statements!)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Create a DecisionTree classifier\n",
    "DT = DecisionTreeClassifier()\n",
    "DT.fit(X_train, y_train)\n",
    "pred = DT.predict(X_test)\n",
    "accDT = accuracy_score(y_test, pred)\n",
    "y_pred_prob = DT.predict_proba(X_test)\n",
    "predT=DT.predict(X_train)\n",
    "aucScoreDT = roc_auc_score(y_test,  y_pred_prob[:,1])\n",
    "fprDT, tprDT, thresholds = roc_curve(y_test, y_pred_prob[:,1] )\n",
    "print(confusion_matrix(y_test,pred))\n",
    "print(classification_report(y_test,pred))\n",
    "print(\"***AUC score***\")\n",
    "print(\"AUC score for DT is \",aucScoreDT)\n",
    "print(\"***Accuracy score***\")\n",
    "print(\"Train Accuracy score for DT is \",accuracy_score(y_train, predT))\n",
    "print(\"Test Accuracy score for DT is \",accuracy_score(y_test, pred))\n",
    "print(\"***Recall score***\")\n",
    "print(\"Train recall score for DT is \",recall_score(y_train, predT))\n",
    "print(\"Test recall score for DT is \",recall_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea44ba9",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc2d662d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6284 1126]\n",
      " [1206 6259]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.84      7410\n",
      "           1       0.85      0.84      0.84      7465\n",
      "\n",
      "    accuracy                           0.84     14875\n",
      "   macro avg       0.84      0.84      0.84     14875\n",
      "weighted avg       0.84      0.84      0.84     14875\n",
      "\n",
      "***AUC score***\n",
      "AUC score for RF is  0.925608087403836\n",
      "***Accuracy score***\n",
      "Train Accuracy score for RF is  1.0\n",
      "Test Accuracy score for RF is  0.8432268907563025\n",
      "***Recall score***\n",
      "Train recall score for RF is  1.0\n",
      "Test recall score for RF is  0.8384460817146685\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Create a RandomForestClassifier classifier\n",
    "RF = RandomForestClassifier()\n",
    "RF.fit(X_train, y_train)\n",
    "pred = RF.predict(X_test)\n",
    "accRF = accuracy_score(y_test, pred)\n",
    "y_pred_prob = RF.predict_proba(X_test)\n",
    "predT=RF.predict(X_train)\n",
    "aucScoreRF = roc_auc_score(y_test,  y_pred_prob[:,1])\n",
    "fprRF, tprRF, thresholds = roc_curve(y_test, y_pred_prob[:,1] )\n",
    "print(confusion_matrix(y_test,pred))\n",
    "print(classification_report(y_test,pred))\n",
    "print(\"***AUC score***\")\n",
    "print(\"AUC score for RF is \",aucScoreRF)\n",
    "print(\"***Accuracy score***\")\n",
    "print(\"Train Accuracy score for RF is \",accuracy_score(y_train, predT))\n",
    "print(\"Test Accuracy score for RF is \",accuracy_score(y_test, pred))\n",
    "print(\"***Recall score***\")\n",
    "print(\"Train recall score for RF is \",recall_score(y_train, predT))\n",
    "print(\"Test recall score for RF is \",recall_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc71631",
   "metadata": {},
   "source": [
    "# XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b99c9a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6181 1229]\n",
      " [ 944 6521]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85      7410\n",
      "           1       0.84      0.87      0.86      7465\n",
      "\n",
      "    accuracy                           0.85     14875\n",
      "   macro avg       0.85      0.85      0.85     14875\n",
      "weighted avg       0.85      0.85      0.85     14875\n",
      "\n",
      "***AUC score***\n",
      "AUC score for XGB is  0.93306736881877\n",
      "***Accuracy score***\n",
      "Train Accuracy score for XGB is  0.9341343244878555\n",
      "Test Accuracy score for XGB is  0.8539159663865546\n",
      "***Recall score***\n",
      "Train recall score for XGB is  0.9497674952637924\n",
      "Test recall score for XGB is  0.8735432016075017\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "# Create a RandomForestClassifier classifier\n",
    "XGB = XGBClassifier()\n",
    "XGB.fit(X_train, y_train)\n",
    "pred = XGB.predict(X_test)\n",
    "accXGB = accuracy_score(y_test, pred)\n",
    "y_pred_prob = XGB.predict_proba(X_test)\n",
    "predT=XGB.predict(X_train)\n",
    "aucScoreXGB = roc_auc_score(y_test,  y_pred_prob[:,1])\n",
    "fprXGB, tprXGB, thresholds = roc_curve(y_test, y_pred_prob[:,1] )\n",
    "print(confusion_matrix(y_test,pred))\n",
    "print(classification_report(y_test,pred))\n",
    "print(\"***AUC score***\")\n",
    "print(\"AUC score for XGB is \",aucScoreXGB)\n",
    "print(\"***Accuracy score***\")\n",
    "print(\"Train Accuracy score for XGB is \",accuracy_score(y_train, predT))\n",
    "print(\"Test Accuracy score for XGB is \",accuracy_score(y_test, pred))\n",
    "print(\"***Recall score***\")\n",
    "print(\"Train recall score for XGB is \",recall_score(y_train, predT))\n",
    "print(\"Test recall score for XGB is \",recall_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74397c4",
   "metadata": {},
   "source": [
    "Logistic regression seems to be best by use of TFIDF vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e06ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e889081",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
